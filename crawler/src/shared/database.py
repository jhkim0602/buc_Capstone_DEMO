import re
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import time

def strip_html(html_content):
    if not html_content:
        return ""
    soup = BeautifulSoup(html_content, "lxml")
    return soup.get_text(separator=" ").strip()

def normalize_url(url):
    try:
        parsed = urlparse(url)
        # ì œê±°í•  íŒŒë¼ë¯¸í„°
        params_to_remove = [
            "utm_source", "utm_medium", "utm_campaign", "utm_content", "utm_term",
            "fbclid", "gclid", "fromRss", "trackingCode", "source", "rss"
        ]

        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì¬êµ¬ì„±
        query_params = []
        if parsed.query:
            pairs = parsed.query.split("&")
            for pair in pairs:
                key = pair.split("=")[0]
                if key not in params_to_remove:
                    query_params.append(pair)

        new_query = "&".join(query_params)

        # ë£¨íŠ¸ê°€ ì•„ë‹Œ ê²½ìš° ê²½ë¡œ ëì˜ ìŠ¬ë˜ì‹œ ì œê±°
        path = parsed.path
        if path.endswith("/") and path != "/":
            path = path[:-1]

        return f"{parsed.scheme}://{parsed.netloc}{path}{'?' + new_query if new_query else ''}"
    except Exception:
        return url

def normalize_title(title):
    if not title:
        return ""
    # ì¤‘ë³µ ê³µë°±ì€ ì •ê·œí™”í•˜ì§€ë§Œ íŠ¹ìˆ˜ ë¬¸ìëŠ” ìœ ì§€
    return re.sub(r'\s+', ' ', title).strip().lower()

def create_summary(content, feed_config=None, entry=None):
    # í•„ìš”í•œ ê²½ìš° Medium ë¡œì§ (JSì—ì„œ ë‹¨ìˆœí™”ë¨)
    if feed_config and "medium.com" in feed_config.get("url", ""):
        # ì½˜í…ì¸ ì—ì„œ íŠ¹ì • ì„¤ëª… ì°¾ê¸° ì‹œë„
        # íŒŒì´ì¬ì˜ ê²½ìš°, ë‹¨ìˆœí•œ html íƒœê·¸ ì œê±° ë° ì˜ë¼ë‚´ê¸° ì‚¬ìš©
        pass

    cleaned = strip_html(content)
    if len(cleaned) > 200:
        return cleaned[:200] + "..."
    return cleaned

def fetch_thumbnail_from_web(url, blog_name="Web"):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            # ì¼ë°˜ì ì¸ ë´‡ ë°©ì§€ ì½”ë“œ
            if response.status_code in [403, 401, 429]:
                 print(f"âš ï¸ [{blog_name} Thumbnail] Access Denied ({response.status_code}): {url}")
            else:
                 print(f"âš ï¸ [{blog_name} Thumbnail] Fetch failed ({response.status_code}): {url}")
            return None

        soup = BeautifulSoup(response.content, "lxml")

        # og:image ë©”íƒ€ íƒœê·¸
        og_image = soup.find("meta", attrs={"property": "og:image"})
        if og_image and og_image.get("content"):
            print(f"âœ… [{blog_name} Thumbnail] Extracted from Web: {og_image['content']}")
            return og_image["content"]

        # twitter:image ë©”íƒ€ íƒœê·¸
        twitter_image = soup.find("meta", attrs={"name": "twitter:image"})
        if twitter_image and twitter_image.get("content"):
             print(f"âœ… [{blog_name} Thumbnail] Extracted from Twitter Meta: {twitter_image['content']}")
             return twitter_image["content"]

        print(f"âŒ [{blog_name} Thumbnail] No meta image found: {url}")
        return None
    except Exception as e:
        print(f"âŒ [{blog_name} Thumbnail] Web fetch failed: {e}")
        return None

def extract_thumbnail(entry, feed_config=None):
    # ì›¹ ìŠ¤í¬ë˜í•‘ì´ í•„ìš”í•œ ë¸”ë¡œê·¸ ëª©ë¡
    web_scraping_domains = [
        "toss.tech", "oliveyoung.tech", "tech.kakao.com", "tech.kakaopay.com",
        "techblog.woowahan.com", "blog.banksalad.com", "tech.devsisters.com",
        "d2.naver.com", "techblog.lycorp.co.jp"
    ]

    link = entry.get("link", "")

    # ìŠ¤í¬ë˜í•‘ì´ í•„ìš”í•œì§€ í™•ì¸
    for domain in web_scraping_domains:
        if domain in link:
            blog_name = feed_config.get("name", "Unknown") if feed_config else "Unknown"
            print(f"ğŸ” [{blog_name} Thumbnail] Attempting web scraping: {link}")
            thumb = fetch_thumbnail_from_web(link, blog_name)
            if thumb:
                return thumb
            break

    # 1. Enclosure (ì²¨ë¶€ íŒŒì¼)
    if "enclosures" in entry:
        for enclosure in entry.enclosures:
            if enclosure.get("type", "").startswith("image/"):
                return enclosure.get("href")

    # 2. Media Content (ë¯¸ë””ì–´ ì½˜í…ì¸ )
    if "media_content" in entry:
        # feedparserëŠ” media:contentë¥¼ media_content ë¦¬ìŠ¤íŠ¸ì— ë„£ìŠµë‹ˆë‹¤
        for media in entry.media_content:
             if "url" in media:
                 return media["url"]

    # 3. Media Thumbnail (ë¯¸ë””ì–´ ì¸ë„¤ì¼)
    if "media_thumbnail" in entry:
        # feedparserëŠ” media:thumbnailì„ media_thumbnail ë¦¬ìŠ¤íŠ¸ì— ë„£ìŠµë‹ˆë‹¤
        if len(entry.media_thumbnail) > 0:
            return entry.media_thumbnail[0].get("url")

    # 4. ë³¸ë¬¸ ì´ë¯¸ì§€ ì¶”ì¶œ
    content = ""
    if "content" in entry and len(entry.content) > 0:
        content = entry.content[0].value
    elif "summary" in entry:
        content = entry.summary
    elif "description" in entry:
        content = entry.description

    soup = BeautifulSoup(content, "lxml")

    # íŠ¹ìˆ˜ ë¡œì§ (ë„¤ì´ë²„, í‹°ìŠ¤í† ë¦¬, ë²¨ë¡œê·¸)
    if "blog.naver.com" in link:
        # ë„¤ì´ë²„ íŠ¹í™” íŒ¨í„´ì€ ì¼ë°˜ ì´ë¯¸ì§€ ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆì§€ë§Œ, ê¼­ í•„ìš”í•œ ê²½ìš° ì œê³µëœ ë¡œì§ì„ ì—ë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤
        # íŒŒì´ì¬ì˜ soup.find_all('img')ê°€ ë” ì‰½ìŠµë‹ˆë‹¤
        pass

    imgs = soup.find_all("img")
    for img in imgs:
        src = img.get("src") or img.get("data-src")
        if not src:
            continue

        if src.startswith("data:"):
            continue

        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not src.startswith("http"):
             src = urljoin(link, src)

        # ë„¤ì´ë²„ íŠ¹í™” ì •ë¦¬
        if "blog.naver.com" in link:
            src = src.split("?")[0]

        return src

    return None
